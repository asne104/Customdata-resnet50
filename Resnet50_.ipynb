{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\0220\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.__version__)\n",
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 라벨 파일 경로 설정\n",
    "data_folder = 'C:/Users/A/Desktop/224_Data'\n",
    "\n",
    "train_data_folder = data_folder + '/Train'\n",
    "val_data_folder = data_folder + '/Validation'\n",
    "test_data_folder = data_folder + '/Test'\n",
    "\n",
    "train_label_file = train_data_folder + '/train_labels.txt'\n",
    "val_label_file = val_data_folder + '/val_labels.txt'\n",
    "test_label_file = test_data_folder + '/test_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate for normalize(min,max)\n",
    "def load_and_combine_data(train_data_folder):\n",
    "    file_paths = [os.path.join(train_data_folder, f) for f in os.listdir(train_data_folder) if f.endswith('9.txt')]\n",
    "    all_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        #Loading arrary data from each txt file\n",
    "        data = np.loadtxt(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "    #combine all data to one file\n",
    "    combined_data = np.concatenate(all_data, axis=0)\n",
    "    return combined_data\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    min = np.min(data)\n",
    "    max = np.max(data)\n",
    "    return min, max\n",
    "\n",
    "combined_data = load_and_combine_data(train_data_folder)\n",
    "matrix_min, matrix_max = calculate_statistics(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1657128 0.07536882537300484\n"
     ]
    }
   ],
   "source": [
    "#check the mean and std \n",
    "print(matrix_min,matrix_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    scaled_matrix = 2 * (matrix - min_val) / (max_val - min_val) - 1  # [0, 1] 범위로 스케일링 후 [-1, 1]로 변환\n",
    "    return scaled_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_folder, label_file):\n",
    "        self.data_folder = data_folder\n",
    "        self.parameters = self.read_parameters(label_file) \n",
    "        self.data_numbers = list(self.parameters.keys())\n",
    "        all_params = np.array(list(self.parameters.values()))\n",
    "        self.label_min = np.min(all_params, axis=0)\n",
    "        self.label_max = np.max(all_params, axis=0)\n",
    "\n",
    "    def read_parameters(self, file_path):\n",
    "        parameters = {}\n",
    "        with open(file_path, 'r') as file:\n",
    "            for index, line in enumerate(file):\n",
    "                if index == 0:  # 첫 번째 줄(헤더) 건너뛰기\n",
    "                    continue\n",
    "                parts = line.strip().split(',')\n",
    "                data_number = parts[0]\n",
    "                params = np.array(parts[1:4], dtype=np.float32)\n",
    "                parameters[data_number] = params\n",
    "        return parameters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_numbers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_number = self.data_numbers[idx]\n",
    "        matrix_path = os.path.join(self.data_folder, f'{data_number}.txt')\n",
    "        matrix = np.loadtxt(matrix_path)  # Load matrix from a txt file\n",
    "        matrix = torch.from_numpy(matrix).float()  # 2D -> 3D tensor\n",
    "\n",
    "        # Normalizing matrix for range [-1, 1]\n",
    "        matrix = scale_matrix(matrix)\n",
    "        params = self.parameters[data_number]\n",
    "        \n",
    "        # label Normalizing => 수정\n",
    "        params = (params - self.label_min) / (self.label_max - self.label_min)\n",
    "        \n",
    "        return matrix.unsqueeze(0), torch.from_numpy(params).float() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터셋 생성\n",
    "train_dataset = CustomDataset(data_folder=train_data_folder, label_file=train_label_file)\n",
    "val_dataset = CustomDataset(data_folder=val_data_folder, label_file=val_label_file)\n",
    "test_dataset = CustomDataset(data_folder=test_data_folder, label_file=test_label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_dataset[50])\\nprint(val_dataset[50])\\nprint(test_dataset[50])\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the custom_dataset\n",
    "'''\n",
    "print(train_dataset[50])\n",
    "print(val_dataset[50])\n",
    "print(test_dataset[50])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolution layer\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\\\n",
    "                     \n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bottleneck architecture\n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "\n",
    "        # Bottleneck Block의 구조\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 3x3 convolution layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        # default values\n",
    "        self.inplanes = 64 # input feature map\n",
    "        self.dilation = 1\n",
    "        # stride를 dilation으로 대체할지 선택\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일 \n",
    "        - 3: 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3 => matrix 사용할거라 1로 변경\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 아래부터 block 형태와 갯수가 ResNet층마다 변화\n",
    "        - self.layer1 ~ 4: 필터의 개수는 각 block들을 거치면서 증가(64->128->256->512)\n",
    "        - self.avgpool: 모든 block을 거친 후에는 Adaptive AvgPool2d를 적용하여 (n, 512, 1, 1)의 텐서로\n",
    "        - self.fc: 이후 fc layer를 연결\n",
    "        \"\"\"\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, # 여기서부터 downsampling적용\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        r\"\"\"\n",
    "        convolution layer 생성 함수\n",
    "        - block: block종류 지정\n",
    "        - planes: feature map size (input shape)\n",
    "        - blocks: layers[0]와 같이, 해당 블록이 몇개 생성돼야하는지, 블록의 갯수 (layer 반복해서 쌓는 개수)\n",
    "        - stride와 dilate은 고정\n",
    "        \"\"\"\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        \n",
    "        # the number of filters is doubled: self.inplanes와 planes 사이즈를 맞춰주기 위한 projection shortcut\n",
    "        # the feature map size is halved: stride=2로 downsampling\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # 블록 내 시작 layer, downsampling 필요\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion # inplanes 업데이트\n",
    "        # 동일 블록 반복\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(resnet50, Bottleneck, layers, pretrained, progress, **kwargs):\n",
    "    r\"\"\"\n",
    "    - pretrained: pretrained된 모델 가중치를 불러오기 (saved by caffe)\n",
    "    - arch: ResNet모델 이름\n",
    "    - block: 어떤 block 형태 사용할지 (\"Basic or Bottleneck\")\n",
    "    - layers: 해당 block이 몇번 사용되는지를 list형태로 넘겨주는 부분\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[resnet50], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet50\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    return Resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_size = 3\n",
    "model_instance = resnet50(pretrained=False)\n",
    "model_instance.fc = nn.Linear(model_instance.fc.in_features, label_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_instance.parameters(), lr=0.0015)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model_instance.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torchsummary import summary\\nsummary(model_instance, (1, 128, 128))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torchsummary import summary\n",
    "summary(model_instance, (1, 128, 128))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size = 64\n",
      "Epoch 1/100\n",
      " Training Loss: 0.6654184964534483\n",
      "  Training R2 scores: [-6.062536776112088, -3.924020007168692, -9.158115698358856]\n",
      " Validation Loss: 0.5612353086471558\n",
      "  Validation R2 scores: [-2.2226573020818745, -2.8281741659213, -9.220138030394507]\n",
      "Epoch 2/100\n",
      " Training Loss: 0.04905787110328674\n",
      "  Training R2 scores: [0.10901838823735321, 0.6436843883856446, 0.7374814080471268]\n",
      " Validation Loss: 0.5907772978146871\n",
      "  Validation R2 scores: [-0.634436803748494, 0.26682558861538896, -18.81911864328063]\n",
      "Epoch 3/100\n",
      " Training Loss: 0.04660401483507533\n",
      "  Training R2 scores: [0.17563831243888417, 0.7195069870073482, 0.7616225267331007]\n",
      " Validation Loss: 0.3050864636898041\n",
      "  Validation R2 scores: [-0.3825733122922992, -0.11025603937773298, -6.794196066317275]\n",
      "Epoch 4/100\n",
      " Training Loss: 0.043013300550611394\n",
      "  Training R2 scores: [0.277639153024391, 0.7081581479604205, 0.7270972619679569]\n",
      " Validation Loss: 0.07375726848840714\n",
      "  Validation R2 scores: [0.2832911666804414, 0.575448953094788, 0.2047012331689455]\n",
      "Epoch 5/100\n",
      " Training Loss: 0.038555097619169636\n",
      "  Training R2 scores: [0.27823745754398554, 0.6979216843743117, 0.8052438720175084]\n",
      " Validation Loss: 0.06507909670472145\n",
      "  Validation R2 scores: [0.20571538154498903, 0.6723486214630877, 0.4996216369199552]\n",
      "Epoch 6/100\n",
      " Training Loss: 0.03648172750284797\n",
      "  Training R2 scores: [0.3434246543508036, 0.7240541134376496, 0.8056754014220564]\n",
      " Validation Loss: 0.04828131447235743\n",
      "  Validation R2 scores: [0.3727503033481082, 0.6480305496991211, 0.5716996312878893]\n",
      "Epoch 7/100\n",
      " Training Loss: 0.04181395735787718\n",
      "  Training R2 scores: [0.3169601198999369, 0.6973562519842558, 0.7082453785479443]\n",
      " Validation Loss: 0.061386313289403915\n",
      "  Validation R2 scores: [0.18148417558936714, 0.6262133118702425, 0.03193316918801681]\n",
      "Epoch 8/100\n",
      " Training Loss: 0.04683749044412061\n",
      "  Training R2 scores: [0.0990436535711896, 0.6914984912926587, 0.7751471720598303]\n",
      " Validation Loss: 0.04462109381953875\n",
      "  Validation R2 scores: [0.39379626773900045, 0.6698930083237307, 0.7225664864724684]\n",
      "Epoch 9/100\n",
      " Training Loss: 0.035960676638703594\n",
      "  Training R2 scores: [0.3912181861922964, 0.7289683973779891, 0.7334530135910206]\n",
      " Validation Loss: 0.06693826367457707\n",
      "  Validation R2 scores: [-0.14516533457110659, 0.36811560227778684, 0.6337509212574621]\n",
      "Epoch 10/100\n",
      " Training Loss: 0.05285368389204929\n",
      "  Training R2 scores: [0.17207274033412512, 0.5231060983572984, 0.6780521941235177]\n",
      " Validation Loss: 0.07644493132829666\n",
      "  Validation R2 scores: [0.30521821939367855, 0.43501688320253873, 0.2344971286487595]\n",
      "Epoch 11/100\n",
      " Training Loss: 0.045213408670143077\n",
      "  Training R2 scores: [0.31044149081459294, 0.6833053549567658, 0.6022732727295474]\n",
      " Validation Loss: 0.2074791043996811\n",
      "  Validation R2 scores: [0.3421973934009883, 0.36023525787400623, -5.166685754449866]\n",
      "Epoch 12/100\n",
      " Training Loss: 0.04507007587112879\n",
      "  Training R2 scores: [0.20582082040433203, 0.6614493203302289, 0.7578472783752677]\n",
      " Validation Loss: 0.060978458573420845\n",
      "  Validation R2 scores: [-0.19236732357450714, 0.7190667720013, 0.5624128098505417]\n",
      "Epoch 13/100\n",
      " Training Loss: 0.03686501154381978\n",
      "  Training R2 scores: [0.3568296962790246, 0.6992457103696145, 0.7972734486249248]\n",
      " Validation Loss: 0.041561007499694824\n",
      "  Validation R2 scores: [0.3733868090017134, 0.7019015215684394, 0.7625889222982514]\n",
      "Epoch 14/100\n",
      " Training Loss: 0.03512370684429219\n",
      "  Training R2 scores: [0.3999212566696865, 0.7146409807523515, 0.8124771861568477]\n",
      " Validation Loss: 0.06604089215397835\n",
      "  Validation R2 scores: [0.2839270130053426, 0.4945452324566949, 0.7691845096126109]\n",
      "Epoch 15/100\n",
      " Training Loss: 0.040959842895206655\n",
      "  Training R2 scores: [0.33802055440719825, 0.6230663009266726, 0.752597014610111]\n",
      " Validation Loss: 0.08532122274239858\n",
      "  Validation R2 scores: [0.2538406979577881, 0.04770839132488691, 0.4264828679177537]\n",
      "Epoch 16/100\n",
      " Training Loss: 0.03382410775674017\n",
      "  Training R2 scores: [0.3311559614941474, 0.8163807978003512, 0.8320153130299656]\n",
      " Validation Loss: 0.06121344864368439\n",
      "  Validation R2 scores: [-0.07446810860166098, 0.6505013732156841, 0.39843941354927404]\n",
      "Epoch 17/100\n",
      " Training Loss: 0.04419614423654581\n",
      "  Training R2 scores: [0.21203009932165862, 0.6821549362736827, 0.7076651376433594]\n",
      " Validation Loss: 0.08272459357976913\n",
      "  Validation R2 scores: [0.10464201054846844, 0.7667419544835284, 0.08594690144688055]\n",
      "Epoch 18/100\n",
      " Training Loss: 0.03630028116075616\n",
      "  Training R2 scores: [0.4086426529032253, 0.7330919217117751, 0.755621593240989]\n",
      " Validation Loss: 0.1737098197142283\n",
      "  Validation R2 scores: [0.3186406444844335, 0.3548020428219977, -3.6673240505752442]\n",
      "Epoch 19/100\n",
      " Training Loss: 0.03292460200425826\n",
      "  Training R2 scores: [0.38332274128136956, 0.7690063885481991, 0.8708714619549509]\n",
      " Validation Loss: 0.06624583899974823\n",
      "  Validation R2 scores: [0.1312178628274301, 0.5930648302051906, 0.7146899795652284]\n",
      "Epoch 20/100\n",
      " Training Loss: 0.040648667239829114\n",
      "  Training R2 scores: [0.3710183702597173, 0.6964876726074196, 0.7419109681980709]\n",
      " Validation Loss: 0.04013410396873951\n",
      "  Validation R2 scores: [0.4068936779140203, 0.7889816030207563, 0.7805173518080055]\n",
      "Epoch 21/100\n",
      " Training Loss: 0.04099559127108047\n",
      "  Training R2 scores: [0.2209494108578346, 0.7387347093327987, 0.8288261821482346]\n",
      " Validation Loss: 0.07806788136561711\n",
      "  Validation R2 scores: [0.307262676238819, 0.5543753165799503, -0.2867186292257151]\n",
      "Epoch 22/100\n",
      " Training Loss: 0.04017218221959315\n",
      "  Training R2 scores: [0.34710895205064507, 0.7475158530010125, 0.63323392706148]\n",
      " Validation Loss: 0.0473720021545887\n",
      "  Validation R2 scores: [0.37573981689611236, 0.6092651918937082, 0.8173430353980857]\n",
      "Epoch 23/100\n",
      " Training Loss: 0.03598733579641894\n",
      "  Training R2 scores: [0.4149937160490479, 0.6169614756232207, 0.8404196059094061]\n",
      " Validation Loss: 0.03829603890577952\n",
      "  Validation R2 scores: [0.461041106274808, 0.7953400904776771, 0.816754642568799]\n",
      "Epoch 24/100\n",
      " Training Loss: 0.032701392511003895\n",
      "  Training R2 scores: [0.43666773300160233, 0.764095295132551, 0.8212933612254251]\n",
      " Validation Loss: 0.15685715526342392\n",
      "  Validation R2 scores: [-0.47183935511684405, 0.5229847950686407, -0.8870802270575859]\n",
      "Epoch 25/100\n",
      " Training Loss: 0.04305062413607773\n",
      "  Training R2 scores: [0.30593058016518493, 0.5366109684830929, 0.8448814910706359]\n",
      " Validation Loss: 0.07935959597428639\n",
      "  Validation R2 scores: [0.22407962936990378, 0.6315015185316999, -0.7182906112528311]\n",
      "Epoch 26/100\n",
      " Training Loss: 0.032098054983898214\n",
      "  Training R2 scores: [0.3985337866004016, 0.81822542737017, 0.8279191976066653]\n",
      " Validation Loss: 0.11337272077798843\n",
      "  Validation R2 scores: [-0.057053038170985504, 0.5025512676677801, -0.3980556290185058]\n",
      "Epoch 27/100\n",
      " Training Loss: 0.03426940945026122\n",
      "  Training R2 scores: [0.4266211258486349, 0.7828164671347795, 0.7334485676827105]\n",
      " Validation Loss: 0.03828764334321022\n",
      "  Validation R2 scores: [0.35913396200608905, 0.6603262977805775, 0.7555989830521115]\n",
      "Epoch 28/100\n",
      " Training Loss: 0.033381883643175424\n",
      "  Training R2 scores: [0.37917625288165424, 0.782732222495538, 0.8234878252336382]\n",
      " Validation Loss: 0.050319733719031014\n",
      "  Validation R2 scores: [0.20490060902361074, 0.40918721721265683, 0.6819481073389582]\n",
      "Epoch 29/100\n",
      " Training Loss: 0.04180179242240755\n",
      "  Training R2 scores: [0.13289229049703455, 0.7712951898378044, 0.8058890726655605]\n",
      " Validation Loss: 0.07282863805691402\n",
      "  Validation R2 scores: [0.3293314052647539, 0.3344037566564598, 0.3431389101489247]\n",
      "Epoch 30/100\n",
      " Training Loss: 0.03223817824925247\n",
      "  Training R2 scores: [0.3914561591240239, 0.7928522377326424, 0.794829387502195]\n",
      " Validation Loss: 0.050060334304968514\n",
      "  Validation R2 scores: [0.14757164102716436, 0.7066748678436722, 0.5919448626484216]\n",
      "Epoch 31/100\n",
      " Training Loss: 0.023975164433451074\n",
      "  Training R2 scores: [0.5207894983961769, 0.8369197176595295, 0.8831900716301178]\n",
      " Validation Loss: 0.062269944697618484\n",
      "  Validation R2 scores: [0.04784856640229995, 0.7622467058500335, 0.7030813505965535]\n",
      "Epoch 32/100\n",
      " Training Loss: 0.03962724126483265\n",
      "  Training R2 scores: [0.36373880102507394, 0.6821480977767385, 0.7399607755856357]\n",
      " Validation Loss: 0.041985103860497475\n",
      "  Validation R2 scores: [0.27552508647743457, 0.7677721424992198, 0.6488593977634348]\n",
      "Epoch 33/100\n",
      " Training Loss: 0.030463494458480886\n",
      "  Training R2 scores: [0.41045373040426014, 0.7929745389027726, 0.8438481808038711]\n",
      " Validation Loss: 0.11587572594483693\n",
      "  Validation R2 scores: [0.10057227229499155, -0.7007545328851341, 0.08369173696640764]\n",
      "Epoch 34/100\n",
      " Training Loss: 0.031300989518824374\n",
      "  Training R2 scores: [0.40130667598247705, 0.7914647456468862, 0.8567033010717234]\n",
      " Validation Loss: 0.1737999568382899\n",
      "  Validation R2 scores: [-0.18568093801213958, -0.6513748701719029, -2.304687208943487]\n",
      "Epoch 35/100\n",
      " Training Loss: 0.03442371989551343\n",
      "  Training R2 scores: [0.31731449310219073, 0.768670631577693, 0.8722893904083868]\n",
      " Validation Loss: 0.05424932514627775\n",
      "  Validation R2 scores: [0.13660610385997063, 0.7303711273353541, 0.7378449650833415]\n",
      "Epoch 36/100\n",
      " Training Loss: 0.03517048237355132\n",
      "  Training R2 scores: [0.20964881897580978, 0.8399357951646775, 0.8814216405299578]\n",
      " Validation Loss: 0.07144080971678098\n",
      "  Validation R2 scores: [0.412564791514624, 0.4364280852322737, 0.2793042419667938]\n",
      "Epoch 37/100\n",
      " Training Loss: 0.03573636200867201\n",
      "  Training R2 scores: [0.3874368363949505, 0.7353253807825038, 0.7852287202068704]\n",
      " Validation Loss: 0.1688693960507711\n",
      "  Validation R2 scores: [0.3586095312158294, 0.08101895900970058, -3.4037958647026905]\n",
      "Epoch 38/100\n",
      " Training Loss: 0.03862756186802136\n",
      "  Training R2 scores: [0.3585781488481913, 0.8080187135664744, 0.6957986578276768]\n",
      " Validation Loss: 0.03365060066183408\n",
      "  Validation R2 scores: [0.395986122007118, 0.7703776115531233, 0.8374234248794842]\n",
      "Epoch 39/100\n",
      " Training Loss: 0.031411409672153626\n",
      "  Training R2 scores: [0.4055153369436749, 0.7919062871280835, 0.8404551550572286]\n",
      " Validation Loss: 0.15034872790177664\n",
      "  Validation R2 scores: [0.43717149894923923, 0.5814251662907053, -3.318245137539072]\n",
      "Epoch 40/100\n",
      " Training Loss: 0.034085692151596673\n",
      "  Training R2 scores: [0.37945880830817813, 0.8090388842370592, 0.7645115493268765]\n",
      " Validation Loss: 0.05010435233513514\n",
      "  Validation R2 scores: [0.47975037045434143, 0.6367598158721222, 0.4393230813087905]\n",
      "Epoch 41/100\n",
      " Training Loss: 0.028789718292261426\n",
      "  Training R2 scores: [0.44531692698956016, 0.7711624233174951, 0.8953322309648687]\n",
      " Validation Loss: 0.029397837196787197\n",
      "  Validation R2 scores: [0.504483457925706, 0.8387302014739924, 0.8541980499897817]\n",
      "Epoch 42/100\n",
      " Training Loss: 0.0343455359535782\n",
      "  Training R2 scores: [0.2402408147790357, 0.8637042942668216, 0.8662404529868044]\n",
      " Validation Loss: 0.14349257200956345\n",
      "  Validation R2 scores: [-0.8608774501677008, 0.18124215346832317, 0.15841364278764447]\n",
      "Epoch 43/100\n",
      " Training Loss: 0.035263182203236376\n",
      "  Training R2 scores: [0.3846097252791234, 0.7714243132731595, 0.7573395121953566]\n",
      " Validation Loss: 0.07658213128646214\n",
      "  Validation R2 scores: [-0.02698765562836991, 0.6100881380897978, -0.26884473894590655]\n",
      "Epoch 44/100\n",
      " Training Loss: 0.03155042633022133\n",
      "  Training R2 scores: [0.33563426154329723, 0.83513886463997, 0.8598620151787737]\n",
      " Validation Loss: 0.052562991778055825\n",
      "  Validation R2 scores: [0.4698600059374307, 0.04923041025906916, 0.6276120112210807]\n",
      "Epoch 45/100\n",
      " Training Loss: 0.03017682827224857\n",
      "  Training R2 scores: [0.3977212650287868, 0.8292788281113002, 0.8833115611948342]\n",
      " Validation Loss: 0.0599309541285038\n",
      "  Validation R2 scores: [0.3171540464048006, 0.7661284686753621, 0.4422306739485635]\n",
      "Epoch 46/100\n",
      " Training Loss: 0.028428505517934497\n",
      "  Training R2 scores: [0.4712004731915652, 0.8503987656171788, 0.839817688829545]\n",
      " Validation Loss: 0.18940617392460504\n",
      "  Validation R2 scores: [-0.4451164775756924, 0.5812882665162058, -2.6456130980500854]\n",
      "Epoch 47/100\n",
      " Training Loss: 0.029109303692453785\n",
      "  Training R2 scores: [0.5018372337043019, 0.8331489135416374, 0.7314546365333386]\n",
      " Validation Loss: 0.046562639996409416\n",
      "  Validation R2 scores: [0.07504720504899964, 0.5054562206393654, 0.8311770024931233]\n",
      "Epoch 48/100\n",
      " Training Loss: 0.024314973846470054\n",
      "  Training R2 scores: [0.5594337275794612, 0.8481158623027569, 0.8489814762026737]\n",
      " Validation Loss: 0.10825695097446442\n",
      "  Validation R2 scores: [-0.044169713059837745, 0.7510555146028985, -1.9367427240281918]\n",
      "Epoch 49/100\n",
      " Training Loss: 0.0319128309033419\n",
      "  Training R2 scores: [0.44760907467556776, 0.8065098651723396, 0.7648873887108487]\n",
      " Validation Loss: 0.17392215629418692\n",
      "  Validation R2 scores: [-0.28137088379615993, 0.21302746353989255, -2.6683026804342025]\n",
      "Epoch 50/100\n",
      " Training Loss: 0.029792951125847667\n",
      "  Training R2 scores: [0.42942538904602623, 0.8154160559928325, 0.8608863205920639]\n",
      " Validation Loss: 0.08196095501383145\n",
      "  Validation R2 scores: [0.0920306848085588, 0.8023290728889994, -0.8026554727716873]\n",
      "Epoch 51/100\n",
      " Training Loss: 0.02993282205180118\n",
      "  Training R2 scores: [0.41673037736995155, 0.8398597864766656, 0.8789446373576263]\n",
      " Validation Loss: 0.03350632078945637\n",
      "  Validation R2 scores: [0.5694177204853843, 0.8761830026886868, 0.7275995476742256]\n",
      "Epoch 52/100\n",
      " Training Loss: 0.024660386831352587\n",
      "  Training R2 scores: [0.5402450982568495, 0.8268812977330853, 0.8770703438166503]\n",
      " Validation Loss: 0.11926912888884544\n",
      "  Validation R2 scores: [-0.5162691701547939, 0.08308753309055883, 0.6483044172335997]\n",
      "Epoch 53/100\n",
      " Training Loss: 0.024944024944775982\n",
      "  Training R2 scores: [0.5136198866979266, 0.8275369568401522, 0.8795341836667936]\n",
      " Validation Loss: 0.024988935018579166\n",
      "  Validation R2 scores: [0.5206250047025147, 0.8960668191738629, 0.8989464810320076]\n",
      "Epoch 54/100\n",
      " Training Loss: 0.022829057354676097\n",
      "  Training R2 scores: [0.5769733373412749, 0.8257437153316713, 0.8817088786264935]\n",
      " Validation Loss: 0.09193884332974751\n",
      "  Validation R2 scores: [-0.46729851348517704, 0.7492194254781432, -0.40563358187533005]\n",
      "Epoch 55/100\n",
      " Training Loss: 0.027614221270931393\n",
      "  Training R2 scores: [0.4370631520807182, 0.8676282617427022, 0.8534510786661248]\n",
      " Validation Loss: 0.11912307391564052\n",
      "  Validation R2 scores: [-0.09558758667946798, -0.7418776792363739, 0.10591554964976868]\n",
      "Epoch 56/100\n",
      " Training Loss: 0.023908681285224463\n",
      "  Training R2 scores: [0.5387607244246106, 0.8788675328643295, 0.8718134138452278]\n",
      " Validation Loss: 0.14986682186524072\n",
      "  Validation R2 scores: [-0.2810735060701932, -1.5574619039739588, 0.547475127591196]\n",
      "Epoch 57/100\n",
      " Training Loss: 0.027649476830112308\n",
      "  Training R2 scores: [0.47152524308435095, 0.8084322679068692, 0.9026766999022969]\n",
      " Validation Loss: 0.05539980592827002\n",
      "  Validation R2 scores: [0.12995686170796494, 0.590066907344974, 0.1841970217978468]\n",
      "Epoch 58/100\n",
      " Training Loss: 0.026043576433470373\n",
      "  Training R2 scores: [0.49628939551672857, 0.8289258927163888, 0.885394448764058]\n",
      " Validation Loss: 0.08609353999296825\n",
      "  Validation R2 scores: [-0.5024896905199772, -0.17714113182444624, 0.6579744815420787]\n",
      "Epoch 59/100\n",
      " Training Loss: 0.02093581722951249\n",
      "  Training R2 scores: [0.5666020908510092, 0.8796156835079192, 0.8995031894157848]\n",
      " Validation Loss: 0.4910558263460795\n",
      "  Validation R2 scores: [-2.8622096443654095, 0.54311704065519, -10.307925512198846]\n",
      "Epoch 60/100\n",
      " Training Loss: 0.021458932443668966\n",
      "  Training R2 scores: [0.5448710125803635, 0.8908114116234009, 0.9235802296587995]\n",
      " Validation Loss: 0.10619732489188512\n",
      "  Validation R2 scores: [-1.312045658682087, 0.1611920220526133, 0.8035858812228763]\n",
      "Epoch 61/100\n",
      " Training Loss: 0.029509213311891807\n",
      "  Training R2 scores: [0.4532834377945789, 0.8649399468889655, 0.8016733604209684]\n",
      " Validation Loss: 0.043546345084905624\n",
      "  Validation R2 scores: [0.49092537802551706, 0.7405314116625679, 0.2285157952385115]\n",
      "Epoch 62/100\n",
      " Training Loss: 0.03414969607011268\n",
      "  Training R2 scores: [0.3102995424834264, 0.8010886148015335, 0.8604247105625731]\n",
      " Validation Loss: 0.07724249052504699\n",
      "  Validation R2 scores: [-0.7632871847541773, 0.24276619806077637, 0.8705566087808705]\n",
      "Epoch 63/100\n",
      " Training Loss: 0.027797011168379532\n",
      "  Training R2 scores: [0.502623578230742, 0.8392182406885125, 0.8687278171906597]\n",
      " Validation Loss: 0.07911556959152222\n",
      "  Validation R2 scores: [0.3948426465182783, 0.3712185105610254, -0.5265195099482367]\n",
      "Epoch 64/100\n",
      " Training Loss: 0.02809935984642882\n",
      "  Training R2 scores: [0.511589319533639, 0.8158535114316761, 0.8144558653349473]\n",
      " Validation Loss: 0.4487617512543996\n",
      "  Validation R2 scores: [0.5382061214825398, 0.6770031191352797, -15.637552143470863]\n",
      "Epoch 65/100\n",
      " Training Loss: 0.023814040579293903\n",
      "  Training R2 scores: [0.5575646085745247, 0.855711054187346, 0.843101177672357]\n",
      " Validation Loss: 0.17115349819262823\n",
      "  Validation R2 scores: [-0.719996800940808, 0.5025015798339861, -1.2523310855266998]\n",
      "Epoch 66/100\n",
      " Training Loss: 0.018887899570951338\n",
      "  Training R2 scores: [0.6143195279844023, 0.8943175924286444, 0.9107605970591393]\n",
      " Validation Loss: 0.1340064729253451\n",
      "  Validation R2 scores: [-0.563967186234521, -0.19995379613841435, 0.4616710749465177]\n",
      "Epoch 67/100\n",
      " Training Loss: 0.01935033451177572\n",
      "  Training R2 scores: [0.6330676295132973, 0.906944101943894, 0.8764725256614039]\n",
      " Validation Loss: 0.037207476794719696\n",
      "  Validation R2 scores: [0.36470301472198297, 0.7167214578176746, 0.6428936915735457]\n",
      "Epoch 68/100\n",
      " Training Loss: 0.016303275172647676\n",
      "  Training R2 scores: [0.6831575025804961, 0.9043668594669912, 0.9044951920548931]\n",
      " Validation Loss: 0.03311766559878985\n",
      "  Validation R2 scores: [0.363540324218244, 0.8620392228068608, 0.6559819346107114]\n",
      "Epoch 69/100\n",
      " Training Loss: 0.01782911314972137\n",
      "  Training R2 scores: [0.625609219999329, 0.9120267772230298, 0.9306221235045326]\n",
      " Validation Loss: 0.5549904008706411\n",
      "  Validation R2 scores: [-10.3392923195047, -2.7932686557429234, -1.2948312079423925]\n",
      "Epoch 70/100\n",
      " Training Loss: 0.021774757555440852\n",
      "  Training R2 scores: [0.6088773113563918, 0.8926211125033618, 0.9039466284831077]\n",
      " Validation Loss: 0.10157717640201251\n",
      "  Validation R2 scores: [-0.11107624419990025, -0.016748494732284458, 0.5070859663725744]\n",
      "Epoch 71/100\n",
      " Training Loss: 0.023706568405032158\n",
      "  Training R2 scores: [0.5467844415366616, 0.8608838335087322, 0.8537361758872969]\n",
      " Validation Loss: 0.19135348995526633\n",
      "  Validation R2 scores: [-1.346748184027187, -0.6165381639607384, 0.25022855209947625]\n",
      "Epoch 72/100\n",
      " Training Loss: 0.02193323574273994\n",
      "  Training R2 scores: [0.6215996662941519, 0.8925396676784715, 0.7758392120329645]\n",
      " Validation Loss: 3.6201272010803223\n",
      "  Validation R2 scores: [-26.648196130700878, -6.178457709935621, -117.18712750296191]\n",
      "Epoch 73/100\n",
      " Training Loss: 0.020089464458195788\n",
      "  Training R2 scores: [0.6183952587109856, 0.8950070219011101, 0.8834830344029707]\n",
      " Validation Loss: 0.7253705163796743\n",
      "  Validation R2 scores: [-14.379779543542785, -5.051254468914003, -2.151884869300353]\n",
      "Epoch 74/100\n",
      " Training Loss: 0.017982885437576396\n",
      "  Training R2 scores: [0.6241621287558081, 0.91431774933883, 0.9242895364103557]\n",
      " Validation Loss: 0.03729825342694918\n",
      "  Validation R2 scores: [0.5345699261907448, 0.6866183269434418, 0.9314647940519928]\n",
      "Epoch 75/100\n",
      " Training Loss: 0.019551151579147892\n",
      "  Training R2 scores: [0.5938656406822298, 0.9102515716613104, 0.9238989804642613]\n",
      " Validation Loss: 0.09568255891402562\n",
      "  Validation R2 scores: [-0.7912582284298022, 0.007649146894134651, 0.514538994002722]\n",
      "Epoch 76/100\n",
      " Training Loss: 0.020732944292065344\n",
      "  Training R2 scores: [0.6087349166156558, 0.879779118682249, 0.870186352920911]\n",
      " Validation Loss: 0.08761432580649853\n",
      "  Validation R2 scores: [-0.10289965446736637, 0.5189068031108548, 0.7601172612089868]\n",
      "Epoch 77/100\n",
      " Training Loss: 0.022264126492174047\n",
      "  Training R2 scores: [0.5791492975110424, 0.842163082169505, 0.9099410475662228]\n",
      " Validation Loss: 0.1253454809387525\n",
      "  Validation R2 scores: [-0.43267429267234037, -0.06302931522387878, 0.4125924451010837]\n",
      "Epoch 78/100\n",
      " Training Loss: 0.019451762745647055\n",
      "  Training R2 scores: [0.6115393840455047, 0.8732693550536053, 0.9448222815412206]\n",
      " Validation Loss: 0.036288318845132984\n",
      "  Validation R2 scores: [0.3530745935354791, 0.9195582839438241, 0.3812061545625668]\n",
      "Epoch 79/100\n",
      " Training Loss: 0.021284228386847598\n",
      "  Training R2 scores: [0.6239275455783183, 0.8863404628260974, 0.8736859764465061]\n",
      " Validation Loss: 0.049028740574916206\n",
      "  Validation R2 scores: [0.1527561831382337, 0.539727081679792, 0.8750628699317988]\n",
      "Epoch 80/100\n",
      " Training Loss: 0.0183848971010823\n",
      "  Training R2 scores: [0.6328014442673937, 0.9128324710139918, 0.9189546433928611]\n",
      " Validation Loss: 0.13625751932462057\n",
      "  Validation R2 scores: [-1.4121047571854417, -0.49870207841909475, 0.5548894855853048]\n",
      "Epoch 81/100\n",
      " Training Loss: 0.018096692124871833\n",
      "  Training R2 scores: [0.6523381098927767, 0.9095473308014494, 0.8639912136334085]\n",
      " Validation Loss: 0.02445106115192175\n",
      "  Validation R2 scores: [0.7056788919731192, 0.8637348872290995, 0.9328997738481547]\n",
      "Epoch 82/100\n",
      " Training Loss: 0.01788746707729603\n",
      "  Training R2 scores: [0.6362858102916351, 0.9013795209046163, 0.9318005951899269]\n",
      " Validation Loss: 0.017203227306405704\n",
      "  Validation R2 scores: [0.7358689537559154, 0.8623199828391348, 0.9466802460366186]\n",
      "Epoch 83/100\n",
      " Training Loss: 0.018965257331728935\n",
      "  Training R2 scores: [0.6045451848014751, 0.8859970213805208, 0.9304095721848137]\n",
      " Validation Loss: 0.09025806685288747\n",
      "  Validation R2 scores: [-0.10306538586106861, 0.35772616510928024, 0.7886030191139696]\n",
      "Epoch 84/100\n",
      " Training Loss: 0.019321567231887264\n",
      "  Training R2 scores: [0.6629291312537913, 0.9059104908343856, 0.8609737130157117]\n",
      " Validation Loss: 0.17337657511234283\n",
      "  Validation R2 scores: [-0.9081935389323854, -0.5856566152000517, -1.3677410305163558]\n",
      "Epoch 85/100\n",
      " Training Loss: 0.0263287805412945\n",
      "  Training R2 scores: [0.5349268309123278, 0.8688603854488495, 0.8537278033029652]\n",
      " Validation Loss: 0.10897129525740941\n",
      "  Validation R2 scores: [0.20160089271969528, 0.30972150237331286, -0.7094486016538384]\n",
      "Epoch 86/100\n",
      " Training Loss: 0.025681196662940477\n",
      "  Training R2 scores: [0.6276718948719109, 0.7271990102975981, 0.8583151267888187]\n",
      " Validation Loss: 0.04862940187255541\n",
      "  Validation R2 scores: [0.2115347465376275, 0.7618694150936742, 0.4541715462869238]\n",
      "Epoch 87/100\n",
      " Training Loss: 0.023437646168627237\n",
      "  Training R2 scores: [0.5892008207763623, 0.774889358864354, 0.9137730291728028]\n",
      " Validation Loss: 0.10669391478101413\n",
      "  Validation R2 scores: [-0.04658943522425174, 0.154713420722278, 0.4028734884922155]\n",
      "Epoch 88/100\n",
      " Training Loss: 0.015248493438488558\n",
      "  Training R2 scores: [0.6890727718647727, 0.9183134076496875, 0.9359394952321648]\n",
      " Validation Loss: 0.07046967248121898\n",
      "  Validation R2 scores: [0.08002871840720782, 0.5334382477785257, 0.7467460641789921]\n",
      "Epoch 89/100\n",
      " Training Loss: 0.019502914902803144\n",
      "  Training R2 scores: [0.6430779907421857, 0.907275644250239, 0.8919880566858569]\n",
      " Validation Loss: 0.15675793339808783\n",
      "  Validation R2 scores: [-0.9730638483346441, 0.039118822241660745, 0.5803868056216368]\n",
      "Epoch 90/100\n",
      " Training Loss: 0.02190932004075301\n",
      "  Training R2 scores: [0.5188258204296772, 0.8975938571623856, 0.9274157879575483]\n",
      " Validation Loss: 0.1696811964114507\n",
      "  Validation R2 scores: [-1.244651631219572, 0.19266770783569664, -0.940268061919695]\n",
      "Epoch 91/100\n",
      " Training Loss: 0.01975914394777072\n",
      "  Training R2 scores: [0.5738686715533519, 0.9071613642255751, 0.9190241910942225]\n",
      " Validation Loss: 0.5727174778779348\n",
      "  Validation R2 scores: [-3.2255381280677184, -1.1529464249844388, -16.568109664139254]\n",
      "Epoch 92/100\n",
      " Training Loss: 0.018800074283621813\n",
      "  Training R2 scores: [0.6143021162358876, 0.9033159520187677, 0.9076634577906058]\n",
      " Validation Loss: 0.23610414564609528\n",
      "  Validation R2 scores: [-1.1728559553282407, 0.12709158586491331, -2.079581591821912]\n",
      "Epoch 93/100\n",
      " Training Loss: 0.016454748074082953\n",
      "  Training R2 scores: [0.7088376845111821, 0.9204648257800062, 0.921608740731017]\n",
      " Validation Loss: 0.17746882513165474\n",
      "  Validation R2 scores: [-1.0389320575812668, -0.6231042270331881, 0.6534997323689821]\n",
      "Epoch 94/100\n",
      " Training Loss: 0.017755539352564437\n",
      "  Training R2 scores: [0.6365735104240633, 0.885655197515078, 0.9222214575081645]\n",
      " Validation Loss: 0.1839552323023478\n",
      "  Validation R2 scores: [-1.0876492907417816, -0.6893941310368159, 0.5895015694705679]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "val_losses = []\n",
    "train_r2_scores_epoch = []  # 훈련 데이터셋에 대한 각 에폭의 R^2 스코어 저장\n",
    "val_r2_scores_epoch = []  # 검증 데이터셋에 대한 각 에폭의 R^2 스코어 저장\n",
    "writer = SummaryWriter()\n",
    "\n",
    "check_batch_size = str(batch_size)  # Check the batch size\n",
    "print('batch size = ' + check_batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_instance.train()  # Training mode\n",
    "    running_loss = 0.0\n",
    "    outputs_list = []  # 에폭별 출력값 저장\n",
    "    labels_list = []  # 에폭별 라벨값 저장\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_instance(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 누적 데이터 저장\n",
    "        outputs_list.append(outputs.cpu().detach().numpy())\n",
    "        labels_list.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    # 에폭별 평균 손실 계산 및 저장\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f' Training Loss: {epoch_loss}')\n",
    "    writer.add_scalar('Training Loss', epoch_loss, epoch)  # Writing TensorBoard\n",
    "\n",
    "    # 에폭별 R^2 스코어 계산\n",
    "    outputs_epoch = np.concatenate(outputs_list, axis=0)\n",
    "    labels_epoch = np.concatenate(labels_list, axis=0)\n",
    "    train_r2_scores = [r2_score(labels_epoch[:, i], outputs_epoch[:, i]) for i in range(labels_epoch.shape[1])]\n",
    "    train_r2_scores_epoch.append(train_r2_scores)\n",
    "    print(f\"  Training R2 scores: {train_r2_scores}\")\n",
    "\n",
    "    # Validation mode\n",
    "    model_instance.eval()\n",
    "    val_running_loss = 0.0\n",
    "    outputs_list = []  # 에폭별 출력값 저장\n",
    "    labels_list = []  # 에폭별 라벨값 저장\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model_instance(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # 누적 데이터 저장\n",
    "            outputs_list.append(outputs.cpu().detach().numpy())\n",
    "            labels_list.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    # 에폭별 평균 손실 계산 및 저장\n",
    "    epoch_val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f' Validation Loss: {epoch_val_loss}')\n",
    "    writer.add_scalar('Validation Loss', epoch_val_loss, epoch)  # Writing TensorBoard\n",
    "\n",
    "    # 에폭별 R^2 스코어 계산\n",
    "    outputs_epoch = np.concatenate(outputs_list, axis=0)\n",
    "    labels_epoch = np.concatenate(labels_list, axis=0)\n",
    "    val_r2_scores = [r2_score(labels_epoch[:, i], outputs_epoch[:, i]) for i in range(labels_epoch.shape[1])]\n",
    "    val_r2_scores_epoch.append(val_r2_scores)\n",
    "    print(f\"  Validation R2 scores: {val_r2_scores}\")\n",
    "\n",
    "print('Finished Training')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_instance.eval()  # 평가 모드로 설정\n",
    "test_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "# 실제 값과 예측 값 저장을 위한 리스트 초기화\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # 기울기 계산 비활성화\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_instance(inputs)\n",
    "        \n",
    "        # MSE 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)  # 배치에 대한 손실을 누적하고, 샘플 수로 가중치를 줌\n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "        # 실제 값과 예측 값을 리스트에 저장\n",
    "        actuals.extend(labels.cpu().numpy())\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "# 전체 테스트 세트에 대한 평균 MSE 계산\n",
    "avg_test_mse = test_loss / total_samples\n",
    "print(f'Test MSE: {avg_test_mse}')\n",
    "\n",
    "# 전체 테스트 세트에 대한 R^2 스코어 계산\n",
    "r2_scores = [r2_score(np.array(actuals)[:, i], np.array(predictions)[:, i]) for i in range(np.array(actuals).shape[1])]\n",
    "for i, r2_score_i in enumerate(r2_scores):\n",
    "    print(f\"  R2 score for label {i}: {r2_score_i}\")\n",
    "print(\"  Mean of R2 score: \", r2_score(np.array(actuals), np.array(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 차원별로 회귀 그래프 그리기\n",
    "num_dimensions = actuals[0].shape[0]  # 라벨 데이터의 차원 수 (여기서는 3)\n",
    "for i in range(num_dimensions):\n",
    "\n",
    "    actuals_i = [x[i] for x in actuals]  # i번째 차원의 실제 값\n",
    "    predictions_i = [x[i] for x in predictions]  # i번째 차원의 예측 값\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "\n",
    "    if i==0:\n",
    "        plt.scatter(actuals_i, predictions_i, alpha=0.5, label=f'Parameter epsilon_0')\n",
    "        plt.title(f'Actual vs. Predicted Values for Parameter epsilon_0')\n",
    "\n",
    "    if i==1:\n",
    "        plt.scatter(actuals_i, predictions_i, alpha=0.5, label=f'Parameter n')\n",
    "        plt.title(f'Actual vs. Predicted Values for Parameter n')\n",
    "\n",
    "    if i==2:\n",
    "        plt.scatter(actuals_i, predictions_i, alpha=0.5, label=f'Parameter K')\n",
    "        plt.title(f'Actual vs. Predicted Values for Parameter K')\n",
    "        \n",
    "    \n",
    "    # 회귀선 그리기\n",
    "    z = np.polyfit(actuals_i, predictions_i, 1)  # 1차원 다항식 계수 찾기\n",
    "    p = np.poly1d(z)  # 1차원 다항식 생성\n",
    "    plt.plot(actuals_i, p(actuals_i), color='red')  # 회귀선 그리기\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
