{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.__version__)\n",
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 라벨 파일 경로 설정\n",
    "data_folder = 'C:/Users/A/Desktop/224_Data'\n",
    "\n",
    "train_data_folder = data_folder + '/Train'\n",
    "val_data_folder = data_folder + '/Validation'\n",
    "test_data_folder = data_folder + '/Test'\n",
    "\n",
    "train_label_file = train_data_folder + '/train_labels.txt'\n",
    "val_label_file = val_data_folder + '/val_labels.txt'\n",
    "test_label_file = test_data_folder + '/test_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate for normalize(min,max)\n",
    "def load_and_combine_data(train_data_folder):\n",
    "    file_paths = [os.path.join(train_data_folder, f) for f in os.listdir(train_data_folder) if f.endswith('9.txt')]\n",
    "    all_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        #Loading arrary data from each txt file\n",
    "        data = np.loadtxt(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "    #combine all data to one file\n",
    "    combined_data = np.concatenate(all_data, axis=0)\n",
    "    return combined_data\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    min = np.min(data)\n",
    "    max = np.max(data)\n",
    "    return min, max\n",
    "\n",
    "combined_data = load_and_combine_data(train_data_folder)\n",
    "matrix_min, matrix_max = calculate_statistics(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the mean and std \n",
    "#print(matrix_min,matrix_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    scaled_matrix = 2 * (matrix - min_val) / (max_val - min_val) - 1  # [0, 1] 범위로 스케일링 후 [-1, 1]로 변환\n",
    "    return scaled_matrix  # 결과를 numpy 배열로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_folder, label_file):\n",
    "        self.data_folder = data_folder\n",
    "        self.parameters = self.read_parameters(label_file) \n",
    "        self.data_numbers = list(self.parameters.keys())\n",
    "        all_params = np.array(list(self.parameters.values()))\n",
    "        self.label_min = np.min(all_params, axis=0)\n",
    "        self.label_max = np.max(all_params, axis=0)\n",
    "\n",
    "    def read_parameters(self, file_path):\n",
    "        parameters = {}\n",
    "        with open(file_path, 'r') as file:\n",
    "            for index, line in enumerate(file):\n",
    "                if index == 0:  # 첫 번째 줄(헤더) 건너뛰기\n",
    "                    continue\n",
    "                parts = line.strip().split(',')\n",
    "                data_number = parts[0]\n",
    "                params = np.array(parts[1:4], dtype=np.float32)  \n",
    "                parameters[data_number] = params\n",
    "        return parameters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_numbers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_number = self.data_numbers[idx]\n",
    "        matrix_path = os.path.join(self.data_folder, f'{data_number}.txt')\n",
    "        matrix = np.loadtxt(matrix_path)  # Load matrix from a txt file\n",
    "        \n",
    "        # 매트릭스 데이터 정규화\n",
    "        matrix_normalized = scale_matrix(matrix)\n",
    "        \n",
    "        params = self.parameters[data_number]\n",
    "        # 레이블 데이터 정규화\n",
    "        params_normalized = (params - self.label_min) / (self.label_max - self.label_min)\n",
    "\n",
    "        return torch.tensor(matrix_normalized).float().unsqueeze(0), torch.from_numpy(params_normalized).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터셋 생성\n",
    "train_dataset = CustomDataset(data_folder=train_data_folder, label_file=train_label_file)\n",
    "val_dataset = CustomDataset(data_folder=val_data_folder, label_file=val_label_file)\n",
    "test_dataset = CustomDataset(data_folder=test_data_folder, label_file=test_label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_dataset[50])\\nprint(val_dataset[50])\\nprint(test_dataset[50])\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the custom_dataset\n",
    "'''\n",
    "print(train_dataset[50])\n",
    "print(val_dataset[50])\n",
    "print(test_dataset[50])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolution layer\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\\\n",
    "                     \n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bottleneck architecture\n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "\n",
    "        # Bottleneck Block의 구조\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 3x3 convolution layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        # default values\n",
    "        self.inplanes = 64 # input feature map\n",
    "        self.dilation = 1\n",
    "        # stride를 dilation으로 대체할지 선택\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일 \n",
    "        - 3: 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3 => matrix 사용할거라 1로 변경\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 아래부터 block 형태와 갯수가 ResNet층마다 변화\n",
    "        - self.layer1 ~ 4: 필터의 개수는 각 block들을 거치면서 증가(64->128->256->512)\n",
    "        - self.avgpool: 모든 block을 거친 후에는 Adaptive AvgPool2d를 적용하여 (n, 512, 1, 1)의 텐서로\n",
    "        - self.fc: 이후 fc layer를 연결\n",
    "        \"\"\"\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, # 여기서부터 downsampling적용\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        r\"\"\"\n",
    "        convolution layer 생성 함수\n",
    "        - block: block종류 지정\n",
    "        - planes: feature map size (input shape)\n",
    "        - blocks: layers[0]와 같이, 해당 블록이 몇개 생성돼야하는지, 블록의 갯수 (layer 반복해서 쌓는 개수)\n",
    "        - stride와 dilate은 고정\n",
    "        \"\"\"\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        \n",
    "        # the number of filters is doubled: self.inplanes와 planes 사이즈를 맞춰주기 위한 projection shortcut\n",
    "        # the feature map size is halved: stride=2로 downsampling\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # 블록 내 시작 layer, downsampling 필요\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion # inplanes 업데이트\n",
    "        # 동일 블록 반복\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(resnet50, Bottleneck, layers, pretrained, progress, **kwargs):\n",
    "    r\"\"\"\n",
    "    - pretrained: pretrained된 모델 가중치를 불러오기 (saved by caffe)\n",
    "    - arch: ResNet모델 이름\n",
    "    - block: 어떤 block 형태 사용할지 (\"Basic or Bottleneck\")\n",
    "    - layers: 해당 block이 몇번 사용되는지를 list형태로 넘겨주는 부분\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[resnet50], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet50\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    return Resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 하이퍼파라미터 제안\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 100, 2000, step=100)\n",
    "\n",
    "    label_size = 3\n",
    "    model_instance = resnet50(pretrained=False)\n",
    "    model_instance.fc = nn.Linear(model_instance.fc.in_features, label_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_instance.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model_instance.to(device)\n",
    "\n",
    "    # 훈련 루프\n",
    "    for epoch in range(num_epochs):\n",
    "        model_instance.train()\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_instance(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 검증 루프\n",
    "    model_instance.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_instance(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:06:43,913] A new study created in memory with name: no-name-7893c974-5782-421f-9d57-36ef0e443642\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_15404\\1497807318.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1000)  # n_trials은 시도할 시행 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = study.best_params['lr']\n",
    "best_num_epochs = study.best_params['num_epochs']\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 설정\n",
    "model_instance = resnet50(pretrained=False)\n",
    "model_instance.fc = nn.Linear(model_instance.fc.in_features, label_size)\n",
    "optimizer = optim.Adam(model_instance.parameters(), lr=best_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "losses = []\n",
    "val_losses = []\n",
    "train_r2_scores_epoch = []  # 훈련 데이터셋에 대한 각 에폭의 R^2 스코어 저장\n",
    "val_r2_scores_epoch = []  # 검증 데이터셋에 대한 각 에폭의 R^2 스코어 저장\n",
    "writer = SummaryWriter()\n",
    "\n",
    "check_batch_size = str(batch_size)  # Check the batch size\n",
    "print('batch size = ' + check_batch_size)\n",
    "\n",
    "best_val_loss = float('inf')  # 검증 손실의 최솟값을 저장할 변수를 초기화 (무한대로 시작)\n",
    "best_model_wts = None  # 최적의 모델 가중치를 저장할 변수\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_instance.train()  # Training mode\n",
    "    running_loss = 0.0\n",
    "    outputs_list = []  # 에폭별 출력값 저장\n",
    "    labels_list = []  # 에폭별 라벨값 저장\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_instance(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 누적 데이터 저장\n",
    "        outputs_list.append(outputs.cpu().detach().numpy())\n",
    "        labels_list.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    # 에폭별 평균 손실 계산 및 저장\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f' Training Loss: {epoch_loss}')\n",
    "    writer.add_scalar('Training Loss', epoch_loss, epoch)  # Writing TensorBoard\n",
    "\n",
    "    # 에폭별 R^2 스코어 계산\n",
    "    outputs_epoch = np.concatenate(outputs_list, axis=0)\n",
    "    labels_epoch = np.concatenate(labels_list, axis=0)\n",
    "    train_r2_scores = [r2_score(labels_epoch[:, i], outputs_epoch[:, i]) for i in range(labels_epoch.shape[1])]\n",
    "    train_r2_scores_epoch.append(train_r2_scores)\n",
    "    print(f\"  Training R2 scores: {train_r2_scores}\")\n",
    "\n",
    "    # Validation mode\n",
    "    model_instance.eval()\n",
    "    val_running_loss = 0.0\n",
    "    outputs_list = []  # 에폭별 출력값 저장\n",
    "    labels_list = []  # 에폭별 라벨값 저장\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model_instance(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # 누적 데이터 저장\n",
    "            outputs_list.append(outputs.cpu().detach().numpy()) \n",
    "            labels_list.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    # 에폭별 평균 손실 계산 및 저장\n",
    "    epoch_val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f' Validation Loss: {epoch_val_loss}')\n",
    "    writer.add_scalar('Validation Loss', epoch_val_loss, epoch)  # Writing TensorBoard\n",
    "\n",
    "    # 에폭별 R^2 스코어 계산\n",
    "    outputs_epoch = np.concatenate(outputs_list, axis=0)\n",
    "    labels_epoch = np.concatenate(labels_list, axis=0)\n",
    "    val_r2_scores = [r2_score(labels_epoch[:, i], outputs_epoch[:, i]) for i in range(labels_epoch.shape[1])]\n",
    "    val_r2_scores_epoch.append(val_r2_scores)\n",
    "    print(f\"  Validation R2 scores: {val_r2_scores}\")\n",
    "    \n",
    "    # 현재 검증 손실이 이전 최소값보다 작으면 모델을 저장\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        print(f\"Saving model from Epoch {epoch+1}\")\n",
    "        best_val_loss = epoch_val_loss  # 최소 검증 손실 업데이트\n",
    "        best_model_wts = model_instance.state_dict().copy()  # 최적의 모델 가중치 저장\n",
    "        torch.save(best_model_wts, 'best_model.pth')  # 모델 가중치를 파일로 저장\n",
    "\n",
    "# 학습이 끝난 후, 최적의 가중치로 모델을 업데이트\n",
    "model_instance.load_state_dict(best_model_wts)\n",
    "\n",
    "print('Finished Training')\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
